1) Для запуска проекта необходимо сначала запустить Docker Compose:
```shell
docker compose up --build
```

2) После чего в логах Docker будет выведена ссылка для подключения к Jupyter Notebook внутри контейнера. Необходимо настроить подключение и зайти в ноутбук `spark/main.ipynb`.
3) В ноутбуке реализована эмуляция запуска **Spark jobs**, в нем необходимо последовательно выполнить каждую ячейку.
4) Также в ноутбуке реализована проверка того, что таблицы успешно загружены в базы, и их вывод.
5) Основной код проекта лежит в директории `spark/`:
```
spark
├── Dockerfile
├── main.ipynb
├── requirements.txt
├── update_data.py
└── update_reports.py
```

6) В `update_data.py` находится код преобразования исходных данных из таблиц `.csv` в модель "снежинка" в **PostgreSQL**.
7) В `update_reports.py` находится код составления и отправки в базы данных отчетов, требуемых в задании.
